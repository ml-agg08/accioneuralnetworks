# **1. Bigraph Language Model in PyTorch**

**A simple implementation of a bigraph language model from scratch, inspired by Andrej Karpathy's tutorial.**

## **Overview**

This project implements a character-level bigraph language model using PyTorch. Itâ€™s a basic model that predicts the next character based on the current one, trained on a dataset of names. Though simple, it served as an excellent introduction to natural language processing (NLP) and PyTorch.

Key Features:
- Understands and generates sequences character-by-character.
- Implements training with gradient descent and manual forward/backward passes.
- Demonstrates the fundamentals of language modeling with minimalistic code.


## **Inspiration**

The idea for this project came after watching Andrej Karpathyâ€™s fantastic tutorial on building language models. His intuitive teaching inspired me to dive into NLP and implement the model entirely from scratch while tweaking and experimenting along the way. 


## **Getting Started**

### **Prerequisites**

- Python 3.x
- PyTorch
- Google Colab (optional but recommended)

### **Running the Code**

1. Clone this repository:
   ```bash
   git clone https://github.com/your-username/bigraph-language-model.git
   cd bigraph-language-model
   ```

2. Open the `.ipynb` notebook file in [Google Colab](https://colab.research.google.com/) or any Jupyter Notebook environment.

3. Run the cells sequentially to train the model and generate sequences.

4. To run the Python script directly:
   ```bash
   python bigraph_model.py
   ```


You're absolutely right! The use of `zip` to create bigraphs is an essential step in the process, and I missed including it in the **Methodology** section. Let me update it for clarity and completeness: 

---

## **Methodology**

### **Dataset**
- A list of names is used as the dataset.
- Each name is prefixed and suffixed with a `.` to mark start and end tokens.

### **Bigraph Creation**
- Bigraphs (pairs of consecutive characters) are created from the dataset using Python's `zip()` function.
- For example, the name `.aarav.` generates bigraphs like `(.a, a.a, a.r, r.a, a.v, v.)`.

### **Encoding**
- Each character in the dataset is one-hot encoded into a 27-dimensional vector (`a-z` + `.`).

### **Model**
- A single-layer model is represented by a weight matrix `W` of size `27x27`, where each row learns the probabilities of transitioning from one character to the next.
- Probabilities are computed using the softmax function applied to the logits.

### **Loss Function**
- Negative Log-Likelihood (NLL) is used to compute the error between the predicted and actual next characters.

### **Optimization**
- Gradient descent is manually implemented to update the weights of the model.

### **Text Generation**
- New sequences are generated by sampling from the learned probabilities for the next character, starting with the special `.` token and continuing until the `.` token is reached again.

## **Results**

### **Sample Generated Outputs**
After training, the model generates character sequences resembling the training data. Examples:

```
Generated Names: ['n', 'i', 'd', 'v', '.']
['s', 'h', 'i', '.']
['s', 'h', 'a', '.']
['p', '.']
['s', 'h', 'a', 'n', 'w', 'o', 'c', 'n', 'a', '.']
['s', 'h', 'i', '.']
['s', 'h', 't', 'o', 'h', 'i', 't', 'u', 's', 'h', 'i', '.']
```


## **Learnings**

This project taught me:
- The fundamentals of character-level language models.
- How to implement matrix-based operations in PyTorch.
- The importance of experimentation and debugging in ML projects.


## **Acknowledgments**

A huge thank you to [Andrej Karpathy](https://youtube.com/@andrejkarpathy) for his insightful tutorial that made this project possible. ðŸ™Œ


## **Contributing**

Feel free to fork this repository and make improvements! Open a pull request to share your ideas.
